#########################################
### IMPORT LIBRARIES AND SET VARIABLES
#########################################
 
#Import python modules
from datetime import datetime
import re
 
#Import pyspark modules
from pyspark.context import SparkContext
import pyspark.sql.functions as f
from pyspark.sql.functions import regexp_extract, col
from pyspark.sql.functions import collect_list
from pyspark.sql.functions import concat_ws
 
#Import glue modules
from awsglue.utils import getResolvedOptions
from awsglue.context import GlueContext
from awsglue.dynamicframe import DynamicFrame
from awsglue.job import Job
 
#Initialize contexts and session
spark_context = SparkContext.getOrCreate()
glue_context = GlueContext(spark_context)
session = glue_context.spark_session
 
#Parameters
glue_db = "clueset-db"
glue_tbl = "read"
s3_write_path = "s3://rohan.aws.glue.test/read"

#########################################
### EXTRACT (READ DATA)
#########################################
 
#Log starting time
dt_start = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
print("Start time:", dt_start)
 
#Read data to Glue dynamic frame
dynamic_frame_read = glue_context.create_dynamic_frame.from_catalog(database = glue_db, table_name = glue_tbl)
print(dynamic_frame_read)
 
#Convert dynamic frame to data frame to use standard pyspark functions
data_frame = dynamic_frame_read.toDF()

#########################################
### TRANSFORM (MODIFY DATA)
#########################################
 
#Create a decade column from year
# rx_guid = re.compile('[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}?')
data_frame.show()
# match = re.search(rx_guid,data_frame["message"].getItem(0).cast("string"))
data_frame = data_frame.withColumn('guid', regexp_extract(col('message'), '([0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}?)', 0))
data_frame.show()
# if (re.search(rx_guid,data_frame["message"]):
    # corr_col
# corr_col = match.group(0)

# decade_col = f.floor(data_frame["year"]/10)*10
# data_frame = data_frame.withColumn("decade", decade_col)

# data_frame = data_frame.withColumn("guid", corr_col)
 
#Group by decade: Count movies, get average rating
# data_frame_aggregated = data_frame.groupby("guid").agg(
# f.count(f.col("movie_title")).alias('movie_count'),
# f.mean(f.col("rating")).alias('rating_mean'),
# )
data_frame_aggregated = data_frame.groupby("guid").agg(collect_list('message').alias("clueSet"))
# data_frame_aggregated.withColumn("clueSet",concat_ws(", ", "clueSet"))#.alias('ClueSet')
# .agg(
# f.count(f.col("message")).alias('clue_count')
# )
 
#Sort by the number of movies per the decade
# data_frame_aggregated = data_frame_aggregated.orderBy(f.desc("movie_count"))
 
#Print result table
#Note: Show function is an action. Actions force the execution of the data frame plan.
#With big data the slowdown would be significant without cacching.
data_frame_aggregated.show(10)

#########################################
### LOAD (WRITE DATA)
#########################################
 
#Create just 1 partition, because there is so little data
data_frame_aggregated = data_frame_aggregated.repartition(1)
 
#Convert back to dynamic frame
dynamic_frame_write = DynamicFrame.fromDF(data_frame_aggregated, glue_context, "dynamic_frame_write")
 
#Write data back to S3
glue_context.write_dynamic_frame.from_options(
frame = dynamic_frame_write,
connection_type = "s3",
connection_options = {
"path": s3_write_path,
#Here you could create S3 prefixes according to a values in specified columns
#"partitionKeys": ["decade"]
},
format = "json"
)
 
#Log end time
dt_end = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
print("Start time:", dt_end)
